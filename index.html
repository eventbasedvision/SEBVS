<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SEBVS: Synthetic Event-based Visual Servoingfor Robot Navigation and Manipulation">
  <meta name="keywords" content="SEBVS">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SEBVS: Synthetic Event-based Visual Servoing
for Robot Navigation and Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./media/css/bulma.min.css">
  <link rel="stylesheet" href="./media/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./media/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./media/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./media/css/index.css">
  <link rel="icon" href="./media/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./media/js/fontawesome.all.min.js"></script>
  <script src="./media/js/bulma-carousel.min.js"></script>
  <script src="./media/js/bulma-slider.min.js"></script>
  <script src="./media/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            SEBVS: Synthetic Event-based Visual Servoing
for Robot Navigation and Manipulation
          </h1>

          <div class="is-size-7 publication-authors">
						<p>(Accepted in 10th International Conference on Computer Vision and Image Processing (CVIP-2025))</p>
					</div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Krishnaa-Vinod">Krishna Vinod</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/Prithvijai">Prithvi Jai Ramesh</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/">Pavan Kumar B N</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/chakravarthi589">Bharatesh Chakravarthi</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Arizona State University</span>
            <span class="author-block"><sup>2</sup>Indian Institute of Information Technology, Sri City, Chittoor</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                 <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> --> 
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/eventbasedvision/SEBVS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://www.dropbox.com/scl/fo/vtwkgit49jqnsafroyzmz/AEcEx-MVqFCMS-1Bi0EHQrI?rlkey=g0t6g5mnymc2wcpx98ypq7812&st=pn805sxx&dl=0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="hero-body">
				<!-- <img src="media/eTraM/gifs/eTramGIF-cropped.gif" class="center"/> -->
				<img src="media/images/fig01-1.png" class="center" />
				<h2 class="subtitles has-text-centered">
					<strong>SEBVS</strong>
				</h2>
			</div>
		</div>
	</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Event cameras have emerged as a powerful sensing modal-
            ity for robotics, offering microsecond latency, high dynamic range, and

            low power consumption. These characteristics make them well-suited
            for real-time robotic perception in scenarios affected by motion blur,
            occlusion, and extreme changes in illumination. Despite this potential,
            event-based vision, particularly through video-to-event (v2e) simulation,
            remains underutilized in mainstream robotics simulators, limiting the
            advancement of event-driven solutions for navigation and manipulation.

            This work presents an open-source, user-friendly v2e robotics operat-
            ing system (ROS) package for Gazebo simulation that enables seamless

            event stream generation from RGB camera feeds. The package is used
            to investigate event-based robotic policies (ERP) for real-time navigation
            and manipulation. Two representative scenarios are evaluated: (1) object
            following with a mobile robot and (2) object detection and grasping with
            a robotic manipulator. Transformer-based ERPs are trained by behavior

            cloning and compared to RGB-based counterparts under various oper-
            ating conditions. Experimental results show that event-based policies

            consistently deliver competitive and often superior robustness in high-
            speed or visually challenging environments. These results highlight the

            potential of event-driven perception to improve real-time robotic navi-
            gation and manipulation, providing a foundation for broader integration

            of event cameras into robotic policy learning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">ERP Archieture </h2>
					<img src="media/images/fig05.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> 
              ERPNav and ERPArm share a lightweight early-fusion transformer. We render event polarity maps (E⁺/E⁻) from the v2e ROS2 node over a short window and stack them with RGB to form a 5-channel tensor. After per-channel normalization, a conv patch-embedding splits the image into tokens; positional encodings are added and the tokens pass through a few self-attention + MLP blocks. A pooled [CLS] token feeds a small policy head: ERPNav regresses (v, ω) for differential drive control, while ERPArm regresses a 6-DoF pre-grasp pose 
(x, y, z, roll, pitch, yaw). Training is behavior cloning from expert demonstrations (navigation: detector-assisted PID producing cmd_vel; manipulation: MoveIt pre-grasp waypoints), using L1/L2 losses with light action smoothing. The design is real-time, requires no depth/flow, and keeps parameters small enough for on-robot inference while retaining the benefits of event-guided perception under fast motion and extreme lighting.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>


  	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">Results</h2>
					<img src="media/images/table_4.png" class="center" />
          <img src="media/images/table_5.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> We evaluated each policy variant over 15 simulated episodes per task. For ERPNav, the RGB+Event model achieved the lowest centroid tracking error (106.7 ± 26.3 px) and the highest success rate (93.3%), while maintaining an appropriate following distance, outperforming RGB-only and Event-only baselines.
 For ERPArm, early fusion likewise delivered the best grasp-ready pose predictions: in single-object scenes it reached 41.1 ± 9.5 mm error, 71.4% accuracy, 7.8 ± 0.6 ms latency, and 51.7% success; in multi-object scenes it achieved 52.6 ± 11.3 mm, 58.9% accuracy, 7.6 ± 0.5 ms latency, and 31.8% success.
 Although Event-only inference was the fastest (≈3.0–3.2 ms), it trailed in accuracy and success, confirming that RGB+Event fusion offers the best overall trade-off between precision, robustness, and responsiveness across both navigation and manipulation tasks.

						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h3 class="title is-3">Results Recordings</h3>
      <div class="publication-videos">
        <div class="video-wrapper">
          <iframe width="560" height="315"
            src="https://www.youtube.com/embed/MVM6hqF_GDs"
            title="YouTube video player"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen>
          </iframe>
        </div>
        <div class="video-wrapper">
          <iframe width="560" height="315"
            src="https://www.youtube.com/embed/UZLcERQZQM8"
            title="YouTube video player"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen>
          </iframe>
        </div>
      </div>
    </div>
    <hr>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{vinod2025sebvs,
  title     = {SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation},
  author    = {Vinod, Krishna and Ramesh, Prithvi Jai and B N, Pavan Kumar and Chakravarthi, Bharatesh},
  booktitle = {},
  year      = {2025}
}
</code></pre>
  </div>
</section>


	<footer class="footer">
		<div class="container">
			<div class="content has-text-centered">
			</div>
			<div class="columns is-centered">
				<div class="column is-8">
					<div class="content">
						<center>
							<p>
								This website is licensed under a <a rel="license"
									href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
									Commons Attribution-ShareAlike 4.0 International License</a>.
								This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
								We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing
								this template.
							</p>
						</center>
					</div>
				</div>
				</p>
			</div>
		</div>
		</div>
		</div>
	</footer>

</body>
</html>
