<!DOCTYPE html>
<html>

<head>

	<meta charset="utf-8">
	<meta name="description" content="InterAct-VideoQA">
	<meta name="keywords" content="InterAct-VideoQA">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:image" content="media/InterAct/InterAct_teaser.png">
	<meta property="og:url" content="https://interact-videoqa.github.io/InterActVideoQA/">
	<meta property="og:description" content="Event-based Traffic Monitoring Dataset">

	<title>
		InterActVideoQA
	</title>

	<!--<link rel="icon" type="image/png" href="media/openscene/logo.png">-->
	<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

	<link rel="stylesheet" href="./media/InterAct/css/bulma.min.css">
	<link rel="stylesheet" href="./media/InterAct/css/bulma-carousel.min.css">
	<link rel="stylesheet" href="./media/InterAct/css/bulma-slider.min.css">
	<link rel="stylesheet" href="./media/InterAct/css/fontawesome.all.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="stylesheet" href="./media/InterAct/css/index.css">

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	<script defer src="./media/InterAct/js/fontawesome.all.min.js"></script>
	<script src="./media/InterAct/js/bulma-carousel.min.js"></script>
	<script src="./media/InterAct/js/bulma-slider.min.js"></script>
	<script src="./media/InterAct/js/index.js"></script>

</head>

<style>
	.publication-videos {
		display: flex;
		flex-wrap: wrap;
		justify-content: center;
	}

	.video-wrapper {
		flex: 0 0 32%;
		padding: 8px;
	}

	.video-wrapper iframe {
		width: 100%;
		height: 240px;
	}
</style>

<body>

	<nav class="navbar" role="navigation" aria-label="main navigation">
		<div class="navbar-brand">
			<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
				<span aria-hidden="true"></span>
				<span aria-hidden="true"></span>
				<span aria-hidden="true"></span>
			</a>
		</div>


		<div class="navbar-menu">
			<div class="navbar-start" style="flex-grow: 1; justify-content: center;">
				<a class="navbar-item" href="https://interact-videoqa.github.io/" target="_blank">
					<span class="icon">
						<i class="fas fa-home"></i>
					</span>
				</a>
				<div class="navbar-item has-dropdown is-hoverable">
					<a class="navbar-link" href="#" target="_blank">
						More Research
					</a>
					<div class="navbar-dropdown">
						<a class="navbar-item" href="https://arxiv.org/abs/2412.01132" target="_blank">
							Eyes on the Road: State-of-the-Art VideoQA Models Assessment for Traffic Monitoring Tasks
						</a>

						<div class="navbar-dropdown">
							<a class="navbar-item" href="https://eventbasedvision.github.io/SEVD/" target="_blank">
								SEVD - CVPRW 2024
							</a>
						</div>
					</div>
				</div>
			</div>
	</nav>


	<section class="hero">
		<div class="hero-body">
			<div class="container is-max-desktop">
				<div class="column has-text-centered">

					<h1 class="title is-2 publication-title">
						InterAct VideoQA: A Benchmark Dataset for Video Question Answering in Traffic Intersection Monitoring
					</h1>

					<div class="is-size-7 publication-authors">
						<p>(Paper under review at a conference location)</p>
					</div>
					<br>
					<div class="is-size-5 publication-authors">


						<span class="author-block">
							<a href="https://github.com/joe-rabbit">Joseph Raj
								Vishal</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						</span>
						<span class="author-block">
							<a href="https://github.com/dbasina">Divesh Basina</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						</span>

						<span class="author-block">
							<a href="https://github.com/KathaNaik/">Katha Naik</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="https://github.com/patilraje">Rutuja Patil</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="https://github.com/manas-1404">Manas Srinivas Gowda</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="https://github.com/Kaiyuan-Tan">Kaiyuan Tan</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="https://chakravarthi589.github.io/">Bharatesh Chakravarthi</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;




					</div>


					<div class="is-size-4 publication-authors">
						<span class="author-block">
							<b> Arizona State University <b>
						</span>
					</div>

					<div class="column has-text-centered">
						<div class="publication-links">

							<!-- 							<span class="link-block">
								<a href="#" target="_blank" class="button is-normal is-rounded is-dark">

									<span class="icon">
										<i class="fas fa-file-pdf"></i>
									</span>

									<span>
										Paper
									</span>

								</a>
							</span> -->

							<!-- 							<span class="link-block">
								<a href="https://interact-videoqa.github.io/InterActVideoQA/" target="_blank"
									class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<i class="ai ai-arxiv"></i>
									</span>

									<span>
										arXiv
									</span>
								</a>
							</span> -->

							<span class="link-block">
								<a href="https://github.com/joe-rabbit/interact_videoqa" target="_blank"
									class="external-link button is-normal is-rounded is-dark">

									<span class="icon">
										<i class="fab fa-github"></i>
									</span>

									<span>
										Code
									</span>
								</a>
							</span>

							<span class="link-block">
								<a href="https://drive.google.com/drive/folders/1dwbeWHASKkLbLOImyHKE8of8hWCq7bdO?usp=sharing"
									target="_blank" class="external-link button is-normal is-rounded is-dark">

									<span class="icon">
										<i class="far fa-images"></i>
									</span>

									<span>
										Data
									</span>
								</a>
							</span>

							<span class="link-block">
								<a href="docs/InterAct_VideoQADatasetDescription.pdf" target="_blank"
									class="external-link button is-normal is-rounded is-dark">

									<span class="icon">
										<i class="far fa-file-alt"></i>
									</span>

									<span>
										Dataset Description
									</span>
								</a>
							</span>

							<br>
						<span class="link-block">
								<a href="https://github.com/joe-rabbit/interact_videoqa/tree/main/Video_Annotation"
									class="external-link button is-normal is-rounded is-dark" target="_blank">

									<span class="icon">
										<i class="fas fa-palette"></i>
									</span>

									<span>
										Annotation Tool
									</span>
								</a>
							</span> 
							<!-- 							<span class="link-block">
								<a href="https://interact-videoqa.github.io/InterActVideoQA/"
									class="external-link button is-normal is-rounded is-dark" target="_blank">

									<span class="icon">
										<i class="fas fa-palette"></i>
									</span>

									<span>
										Poster
									</span>
								</a>
							</span> -->


							<!-- <span class="link-block">
								<a href="https://interact-videoqa.github.io/InterActVideoQA/"
									class="external-link button is-normal is-rounded is-dark">

									<span class="icon">
										<i class="fab fa-youtube"></i>
									</span>

									<span>
										Video
									</span>
								</a>
							</span> -->


							<!-- 							<span class="link-block">
								<a href="https://interact-videoqa.github.io/InterActVideoQA/" target="_blank"
									class="button is-normal is-rounded is-dark" target="_blank">
									<span class="icon">
										<i class="fas fa-file-pdf"></i>
									</span>

									<span>
										Supplementary Material
									</span>
								</a>
							</span> -->

						</div>
					</div>
				</div>
			</div>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="hero-body">
				<!-- <img src="media/eTraM/gifs/eTramGIF-cropped.gif" class="center"/> -->
				<img src="media/InterAct/InterAct_teaser.png" class="center" />
				<h2 class="subtitles has-text-centered">
					<strong>InterAct VideoQA</strong> is a curated, publicly available traffic monitoring dataset gathered using
					ARGOS Cameras and mobile devices under diverse weather and lighting conditions. It comprises 8 hours of
					real-world footage from multiple intersections, segmented into 10-second clips, and features over 25,000
					question-answer pairs covering spatiotemporal dynamics, vehicle interactions, and incident detection. This
					dataset enables the benchmarking and enhancement of VideoQA models for intelligent transportation systems. It
					includes five QA types: (1) attribution, (2) counting, (3) event reasoning, (4) reverse reasoning, and (5)
					counterfactual inference.
				</h2>
			</div>
		</div>
	</section>

	<!-- <section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="hero-body" style="padding: 0px;">
				<center>
					<h3 class="title is-3" style="margin-bottom: 0px;">Teaser Video</h3>
					<iframe src="https://arizonastateu-my.sharepoint.com/personal/averma90_sundevils_asu_edu/_layouts/15/embed.aspx?UniqueId=4fe2ce95-9fa7-4217-8c73-66301672992b" width="640" height="360" frameborder="0" scrolling="no" allowfullscreen title="eTramGIF.gif"></iframe> 
					<iframe width="640px" height="360px"
						src="https://www.youtube.com/embed/uhpCzhFRxdY?si=2wR8-fdWSESWyoMn&mute=1&autoplay=1&loop=1"
						title="InterAct Teaser" frameborder="0"
						allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
						referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
			</div>
		</div>
	</section> -->

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<h2 class="title is-3">Abstract</h2>
					<div class="content has-text-justified">
						<p>Traffic monitoring is crucial for urban mobility, road safety, and intelligent transportation systems
							(ITS). Deep learning has advanced video-based traffic monitoring through video question answering
							(VideoQA) models, enabling structured insight extraction from traffic videos. However, existing VideoQA
							models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold
							across spatiotemporal dimensions. To address these challenges, this paper introduces InterAct VideoQA, a
							curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks. The
							InterAct VideoQA dataset comprises 8 hours of real-world traffic footage collected from diverse
							intersections, segmented into 10-second video clips, with over 25,000 question-answer (QA) pairs covering
							spatiotemporal dynamics, vehicle interactions, incident detection, and other critical traffic attributes.
							State-of-the-art VideoQA models are evaluated on InterAct VideoQA, exposing challenges in reasoning over
							fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these
							models on InterAct VideoQA yields notable performance improvements, demonstrating the necessity of
							domain-specific datasets for VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
							facilitate future research in
							real-world-deployable VideoQA models for intelligent transportation systems.
						</p>
					</div>
				</div>
			</div>

		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">InterAct - Data Collection Setup and Diversity </h2>
					<img src="media/InterAct/InterAct_Collection.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> Overview of the InterAct VideoQA data collection framework, integrating traffic video recording and
							processing with a
							hybrid approach combining manual labeling and GPT-based automation. The pipeline segments eight hours of
							footage into 10-second clips, extracts key metadata (e.g., vehicle attributes, movement patterns,
							pedestrian data), and generates structured question-answer pairs covering attribution, counting, reverse
							reasoning, event reasoning, and counterfactual inference.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>


	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">

					<h2 class="title is-3">InterAct - Statistics </h2>
					<img src="media/InterAct/InterAct_Stats.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> Overview of the InterAct VideoQA dataset, which comprises 28,800 question-answer pairs across various
							reasoning categories. A higher concentration appears in counting, attribute recognition, and event
							reasoning, followed by counterfactual inference and reverse reasoning (3a). Figures 3(b)-(d) illustrate
							the dataset's emphasis on vehicular-related questions, the dominance of attribution and event reasoning
							categories, and the distribution of question types (“what,” “where,” and “how”). This structured approach
							supports the analysis of complex, multi-event traffic scenarios, requiring robust spatio-temporal
							reasoning. A rigorous human and GPT-assisted validation process ensures the consistency, accuracy, and
							reliability of all annotations.

						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">

					<h2 class="title is-3">InterAct - Experimentation Statistics </h2>
					<img src="media/InterAct/InterAct_Model_Stats.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> Performance analysis of VideoLlama2, Llava-NeXT-Video, and Qwen2-VL-7B-hf on the InterAct VideoQA
							dataset, highlighting metric distributions (a), before vs. after fine-tuning (b), and multi-metric
							improvements (c). Notably, <strong>Qwen2-VL-7B-hf</strong> demonstrates the most substantial gains across
							complex reasoning tasks, emphasizing the effectiveness of fine-tuning for robust traffic video analysis.

						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop" style="max-width: 200%; width: 1200px;">
			<div class="hero-body">
				<center>
					<h3 class="title is-3">Sample Data Recordings</h3>
					<div class="publication-videos">
						<div class="video-wrapper">
							<iframe src="https://www.youtube.com/embed/zctaHOSsrXE?si=bF_imjL0QqvdZc1L"
								allow="accelerometer; autoplay; encrypted-media; gyroscope; web-share" allowfullscreen></iframe>
						</div>
						<div class="video-wrapper">
							<iframe src="https://www.youtube.com/embed/cpvGMZYcH24?si=o7q2rowf6SvWgBm-"
								allow="accelerometer; autoplay; encrypted-media; gyroscope; web-share" allowfullscreen></iframe>
						</div>
						<div class="video-wrapper">
							<iframe src="https://www.youtube.com/embed/SSjnZOGdXfM?si=amTy_vwWz5WImBWt"
								allow="accelerometer; autoplay; encrypted-media; gyroscope; web-share" allowfullscreen></iframe>
						</div>
					</div>
				</center>
			</div>
			<hr>
		</div>
	</section>


	<section class="hero teaser" id="BibTeX">
		<div class="container is-max-desktop content">
			<center>
				<h2 class="title">BibTeX</h2>
			</center>
			<pre></pre>
		</div>
	</section>

	<!--<section class="section" id="Acknowledgements">
		  <div class="container is-max-desktop content">
			<h2 class="title">Acknowledgements</h2>
			We sincerely thank Golnaz Ghiasi for providing guidance of using OpenSeg model. We also thank Huizhong Chen, Yin Cui, Tom Deurig, Dan Gnanapragasam, Xiuye Gu, Leonidas Guibas,
		Nilesh Kulkarni, Abhijit Kundu, Hao-Ning Wu, Louis Yang, Guandao Yang, Xiaoshuai Zhang, Howard Zhou, and Zihan Zhu for helpful discussion. We are thankful for the proofreading by Charles R. Qi and Paul-Edouard Sarlin.
			The project logo was created by <a href="https://www.flaticon.com/free-icon/door_2237440?term=door&page=1&position=2&page=1&position=2&related_id=2237440&origin=tag" title="door icons">Door icons created by Good Ware - Flaticon</a>.
		  </div>
		</section>-->


	<footer class="footer">
		<div class="container">
			<div class="content has-text-centered">
			</div>
			<div class="columns is-centered">
				<div class="column is-8">
					<div class="content">
						<center>
							<p>
								This website is licensed under a <a rel="license"
									href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
									Commons Attribution-ShareAlike 4.0 International License</a>.
								This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
								We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing
								this template.
							</p>
						</center>
					</div>
				</div>
				</p>
			</div>
		</div>
		</div>
		</div>
	</footer>

</body>

</html>